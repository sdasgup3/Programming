Compilers for statically typed languages rely on type information to generate
efficient machine code. In a dynamically typed programming language such as
JavaScript, the types of expressions may vary at runtime. 
Without exact type information, the compiler
must emit slower generalized machine code that can deal with all potential type
combinations. 

While compile-time static type inference might be able to
gather type information to generate optimized machine code, 
       but it is too enpensive.
----------------------------------
Identify frequently executed loops ``hot loops'' paths on the fly.
which is called T.
Once the trace is identifies they try to compile it with the specific
type info pertaining to that T.
In there imp they specifically deal with types trace
type trace is a trace with type of the var annotated on the trace.

In the implementation they is a type map for each trace
which for each variable giving the required types for all the variabls used on the trace.
x:int b:bool: the trace can be entered if the value of the variable
x is int and ...

Which is the expected types of all the variables  before entering the loop.

In the subsequesnt iterations the compiled trace can be executed. The
execution is based on the speculation that the same path will be followed
every time or the types of the values will remain the same as we discovered
during compiling.

So to prevent that traces will be having guards which checks
if any of the speculation has failed. If yes trcae will exit

----------------------------------

It records the call of the inner trace T1



--------------------------------
All the traces starts at the loop header
easy to recognize as a bc is lh if its the target of a be.
when the lh become hot recording starts.

each lh is associated with a type map so there could be many
traces for a given lh

type stable traces: when the trace enters the lh with the same tm
as entry tm. 

in this case the end of the trace can jump directly to the lh.

entry typemap: contain the type of the varibales before the loop
is entered.

Type unstable traces:
when the trace enters the lh with diff tm than the entry tm. 

when this can happen: This can happen during the first iter of the 
loop when a var which was initially undefined is bound to some concrete type inside
the loop. when the recording comes back to the lh there is a type mismatch
and the recording will get  a side exit. Still this trace will be compiled 
and every time it will get a side exit.

Do we need to wait for side exit to be come hot???  yes

At the same time a new trace will be recorded with the 
new tm. 
This is a overhead as every time we start recording and compiling and end up getting a 
side exit to I. It would have been better to just interpret instead.

So next time when we are getting such a side exit due to type mismatch
we check the entry map of all existing trace for that loop in case they complement each other.


if the loop is exited because of return or break. the VM will not extend (or further record).

Eg. when they observe that a number type variable holds an interger value at the statrt of 
the loop. SO the entry map will have the variable typed as integer.
But during trace recording the variable is assined non-int values. So
side exit.
But also this info is recorded in an oracle that the type
of the variable may change. Second time we come for recording
we start with the double value. [second time because we did notstarted recording the 
new trace with double as ype map until that side exot become hot]


Extending:
WHen ever there is a side exit due to diff paths are taken or paths with diff types or object rep 
are seen, there is a side exit. The recording starts wwhen the SE become hot.
The trace will end at the loop header of the root trace.

Extend only for control flow diver or when the loop is not exited.
For tyoeunstatble loops we re not extending but creating a diff root trace.






Also the vm instructions are portable
\item
Compiler does not have many information like ...
A JIT compiler runs after the program has started and compiles the code
(usually bytecode or some kind of VM instructions) on the fly (or just-in-time,
    as it's called) into a form that's usually faster, typically the host CPU's
native instruction set. A JIT has access to dynamic runtime information whereas
a standard compiler doesn't and can make better optimizations like inlining
functions that are used frequently.

\item In a stack based machine, opeartions are done by pshing operands onto the stack and then performing the opeartions 
on the opearnds on the top of the stack. Stack based architecture got revived with the intro of
JVM whih is a software interpretter for java bytecodes (an intermedate lang prducrd by java compiler)
An interpretter provides software compatibility across multiple 
platforms. TO overcome the high perf pnalty of interpretation JIT compilers are creted.

\item for dynamically typed language interpretation is important and to overcome 
the limitations of interpretation.

\item (HOW)In practice, methods are not compiled the first time they are called. For
each method, the JVM maintains a call count, which is incremented every time
the method is called. The JVM interprets a method until its call count exceeds
a JIT compilation threshold. Therefore, often-used methods are compiled soon
after the JVM has started, and less-used methods are compiled much later, or
not at all. The JIT compilation threshold helps the JVM start quickly and still
have improved performance. The threshold has been carefully selected to obtain
an optimal balance between startup times and long term performance.

After a method is compiled, its call count is reset to zero and subsequent
calls to the method continue to increment its count. When the call count of a
method reaches a JIT recompilation threshold, the JIT compiler compiles it a
second time, applying a larger selection of optimizations than on the previous
compilation. This process is repeated until the maximum optimization level is
reached. The busiest methods of a Java program are always optimized most
aggressively, maximizing the performance benefits of using the JIT compiler.
The JIT compiler can also measure operational data at run time, and use that
data to improve the quality of further recompilations

This is in contrast to a traditional compiler that compiles all the code to machine language before the program is first run.
\end{itemize}

\chapter{SLIDE 1}
Dynamic languages like javascripts are popular: expressie, accessible to non
experts and make deployment easy as dstributing a source file.  BUT more
difficult to compile than statically typed languages. Even static type
inferencing is too expensive to help for highly interactive environment



\end{itemize}

\section{static vs dynamic type}

\begin{itemize}
\item

In statically types languages, a variable is bound to a type at compile time.
At run time the variable can bound to objects of the same type.

In dynamically typed languages, the variables are notbound to a specific 
type during compile time. At run time, the variable can bind to objects of
diff types.


In a \textbf{statically typed language}, every variable name is bound 
to a type (at compile time, by means of a data declaration)
(The binding to an object is optional — if a name is not bound to an object, the name is said to be null.)

Once a variable name has been bound to a type (that is, declared) it can be
bound (at run time) (via an assignment statement) only to objects of that type; it cannot
ever be bound to an object of a different type. An attempt to bind the name to
an object of the wrong type will raise a type exception. Also
a type specified what operation that u are permitted to do on that varible

In a \textbf{dynamically typed language}, every variable name  are bound to objects at execution time by
  means of assignment statements, and it is possible to bind a name to objects
  of different types during the execution of the program.

  Python is a dynamically-typed language. 
  Java is a statically-typed language.

\item
In a weakly typed language, variables can be implicitly coerced to unrelated
types (like ints and strings), whereas in a strongly typed language they cannot, and an explicit
conversion is required. 

(Note that I said unrelated types. Most languages will
    allow implicit coercions between related types — for example, the addition
    of an integer and a float. By unrelated types I mean things like numbers
    and strings.)

In a typical weakly typed language, the number 9 and the
string “9” are interchangeable, and the following sequence of statements is
legal.

a  = 9
b = "9"
c = concatenate(a, b)  // produces "99"
d = add(a, b)          // produces 18
In a strongly typed language, on the other hand, the last two statements would
raise type exceptions. To avoid these exceptions, some kind of explicit type
conversion wt
  \end{itemize}  

\chapter{Evaluation}

\section{Results}
    \item TraceMonkey is the fastest on 9 out of 26. 
    because some of them are short programs with integer operations, so TM covers the entire prgm with 1-2 traces with integer type.
    All other programs in this set is run almost entirely as native code.
    \item 
    These estimates also indicate that our startup performance could
be substantially better if we improved the speed of trace recording
and compilation. The estimated 200x slowdown for recording and
compilation is very rough, and may be influenced by startup factors
in the interpreter (e.g., caches that have not warmed up yet during
recording). One observation supporting this conjecture is that in
the tracer, interpreted bytecodes take about 180 cycles to run. Still,
recording and compilation are clearly both expensive, and a better
implementation, possibly including redesign of the LIR abstract
syntax or encoding, would improve startup performance.
  \end{itemize}




\chapter{References}

\section{Trace optimization for dynamic languages}
To our knowledge, Rigo’s Psyco (16) is the only published
type-specializing trace compiler for a dynamic language (Python).
Psyco does not attempt to identify hot loops or inline function calls.
Instead, Psyco transforms loops to mutual recursion before running
and traces all operations.




trace recording and patching for non loop and nested loop case



\end{document}

/cmnt{
Just-in-time compilers are often quite similar in structure to their
static counterparts. While they may be employing techniques
specifically to reduce compilation time 
(for example, using linear-
scan register allocation instead of a graph-coloring algorithm), 
they
typically start off by constructing a control-flow graph (CFG) for
the code to be compiled, then perform a series of optimization steps
based on this graph, and as a final step traverse the CFG and emit
native code. 
In addition to a simple CFG, more ambitious opti-
mizing compilers often use an intermediate representation based
on Static Single Assignment (SSA). 

They explore a different approach to building com-
pilers in which no CFG is ever constructed. Instead, our compiler
records and generates code from dynamically recorded code traces.
Each code trace represents a loop in the program and may poten-
tially span several basic blocks across several methods.

Definition of trace:
}

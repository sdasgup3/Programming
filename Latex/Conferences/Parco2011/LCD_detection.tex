% -*- LaTeX -*-
\documentclass{IOS-Book-Article}

\usepackage{times}
\normalfont
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{url}
\usepackage{pstricks}
\usepackage{pst-node}
\usepackage{pst-rel-points}
\usepackage{flexiprogram}
\newtheorem{example}{Example}

\newcommand{\ak}[1]{{\blue #1}}
\newcommand{\sd}[1]{{\green #1}}
\newcommand{\bb}[1]{{\red #1}}
\newcommand{\loc}{\ensuremath{l_0}}
\newcommand{\mynext}{\mbox{\tt next}}
\newcommand{\lt}{\mbox{\tt left}}
\newcommand{\rt}{\mbox{\tt right}}
\newcommand{\isInterfering}{\mbox{isInterfering}}
\newcommand{\interf}[2]{\mbox{\sf interfere}(#1,\ #2)}
\bibliographystyle{plain}
\begin{document}
\begin{frontmatter}                           % The preamble begins here.

%\pretitle{Pretitle}
\title{Heap Dependence Analysis for Sequential Programs}
%
\subtitle{An extended abstract}

\author{\fnms{Barnali} \snm{Basak}},
\author{\fnms{Sandeep} \snm{Dasgupta}}
and
\author{\fnms{Amey} \snm{Karkare}}

\address{{\tt \{barnali, dsand, karkare\}@cse.iitk.ac.in}\\
  Department of CSE, IIT Kanpur, India}

\begin{abstract}
In this paper we demonstrate a novel technique for detecting
heap dependences in sequential programs that uses recursive
data structures.  The novelty of our technique lies in the
way we compute, for each statement, abstract {\em heap access
  paths} that approximate the locations accessed by the
statement, and the way we convert these paths into equations
that can be solved using traditional tests, e.g. GCD test,
Banerjee test and Lamport test.  The dependence test also
uses a field sensitive shape analysis to detect dependences
among heap locations arising out of sharing within the data
structure. In presence of loops, the technique can be used to
discover {\em loop dependences}.  i.e. the dependence among
two different iterations of the same loop. This information
can be used by a parallelizing compiler to transform
sequential input program for better parallel execution.
\end{abstract}

\begin{keyword}
Dependence Analysis\sep
Shape Analysis \sep {Parallelizing
  Compilers} \sep {Recursive Data Structures}
\end{keyword}
\end{frontmatter}

\thispagestyle{empty}
\pagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

In the recent arena of parallel architectures (multi-cores,
GPUs, etc.), software side lags behind hardware in terms of
parallelism. Parallelization of sequential programs, without
violating their correctness, is a key step in increasing
their performance and efficiency. Over the past years, lot of
work has been done on automatically parallelizing sequential
programs. These approaches have mainly been developed for
programs written in languages, such as FORTRAN, having only
static data structures (fixed sized
arrays)~\cite{Allen87automatic,Banerjee93automatic,Wolf91loop,
  Kennedy01Optimizing}.  Almost all programming languages
today use the heap for dynamic memory structures. Therefore,
any parallelization must also take into account the data
dependency due to the access of common heap
locations. Finding parallelism in sequential programs written
in languages with dynamically allocated data structures, such
as C, C++, JAVA, LISP etc., has been less successful. One of
the reason being the presence of pointer-induced aliasing,
which occurs when multiple pointer expressions refer to same
storage location. Compared to the analysis of static and
stack data, analyzing properties of heap data is challenging
because the structure of heap is unknown at compile time, it
is also potentially unbounded and the lifetime of a heap
object is not limited by the scope that creates it. As a
consequence, properties of heap (including dependence) are
approximated very conservatively. The approximation of the
heap dependence information inhibits the parallelization. The
following example motivates the need for a precise dependence
analysis.
  \begin{figure}[t]
    \centering
    \input{fig_motivational_example}
    \label{fig:motiv}
    \caption{A motivating example}
    \hrule
  \end{figure}
\begin{example}{\rm
  Figure~\ref{fig:motiv}
  shows a data structure and the code fragment traversing the
  data structure. The performance of the code can be improved if the loop can
  be executed in parallel. However, without the knowledge of
  precise heap dependences, we have to assume worst case
  scenario, i.e.,  the  location read by the statement {\tt
    S3} in some iteration could be the same as the location
  written by the statement {\tt S5} in some other iteration.
  In that case,  it is not possible to parallelize the loop.
 
  Our dependence analysis can show that the locations read by
  {\tt S3} and those written by {\tt S5} are mutually
  exclusive. Further, it also shows the absence of any other
  dependences.  This information, along with the information
  from classical control and data dependence analysis, can be
  used by a parallelizing compiler to parallelize the loop.
}
\hfill\psframebox{}  \end{example}

The rest of this paper explains our approach for a practical
heap data dependence analysis. As it is understood that we
are only talking about data dependences, we drop the term
data in the rest of the paper. We first present a shape
analysis framework that is used by our analysis to detect
sharing (also called interference) among the data
structures created on the heap.  The details of our
dependence detection framework are explained next.  We then
present our method to handle loops in a more precise way.  We
finish the paper by giving the directions for the future
research.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Shape Analysis Framework}
The goal of our shape analyzer is to detect the shape of the
data structure pointed to by the heap directed pointers at
each program point. Our approach is similar to the work
proposed by Ghiya et. al.~\cite{Ghiya96Tree} in that it also
uses the {\em Direction Matrix} and the {\em Interference
  Matrix} to keep track of shapes of data
structures. However, our matrices also store additional
information about abstract paths between two heap nodes. In
particular, our shape analysis is {\em field sensitive}; it
remembers fixed length prefixes (sequence of field names) of
the paths between two heap nodes. This helps us detect the
shape transitions where a cyclic structure becomes acyclic at
some program point. The later happens due to destructive
updates through statements like {\tt p $\rightarrow$ f =
  NULL} or {\tt p $\rightarrow$ f = q}. This is an improvement
over earlier approaches where once a data structure is marked
cyclic, it remains so for the rest of the analysis.

For dependence detection, our shape analysis framework
provides an interface function {\tt \isInterfering({\tt p},
  $\alpha$, {\tt q}, $\beta$)}. For heap pointers {\tt p, q} and
field sequences $\alpha, \beta, \alpha', \beta'$, this function returns true if the
paths {\tt p.$\alpha'$} and {\tt q.$\beta'$} interfere
(potentially reach the same heap node at run-time), and
$\alpha$ and $\beta$ are prefixes of $\alpha'$ and $\beta'$
respectively.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dependence Analysis}

Two statements are said to be heap dependent on each other if
both statements access the same heap location and at least
one of the statements writes to that location. We have
developed a novel technique which finds out heap induced
dependencies, between any two statements in the program. The
novelty of our approach lies in the separation of shape
analysis phase from the dependence detection phase For each
statement our analysis computes two sets of heap access paths
(a) {\em Read set}: the set of paths which are accessed to
read a heap location, and (b) {\em Write set}: the set of
paths which are accessed to write a heap location.

Our approach is conservative in the sense that the read set
and write set we compute for a statement are over
approximations of the actual locations that are read or
written by the statement. Therefore it is possible that our
analysis reports two statement to be dependent when they are
not really dependent on each other. However, this can inhibit
some parallelizing optimization but can not result in an
incorrect parallelization. Function calls are handled by
using conservative read/write sets that over approximate the
heap locations that could potentially be read or written
inside the called function.
\begin{figure}
%\hrule
{\tt
  \begin{program}{0}
  \FL analyze(f, k)  \{
  \UNL{0} initialize(); \COMMENT{Initialize all parameters and
  globals}
  \UNL{0} States = computeStates(k)
  \UNL{0} computeReadWrite(States)
  \UNL{-1} \}
  \end{program}
}
%\hrule
  \caption{Algorithm to analyze a function {\tt f} for
    dependence detection. Parameter {\tt k} is used for
    limiting the length of access paths, to keep the analysis
    bounded. \label{fig:algoTopLevel}}
\hrule  
\end{figure}

Figure~\ref{fig:algoTopLevel} gives top level pseudo code for
analyzing a function {\tt f} in the program. The functions used by
the code are described below.

Function {\tt initialize()} initializes the
  parameters of the function and global variables with
  symbolic values. The initialization information is also
  accessible to the shape analyzer that requires it for
  making interference decisions.
  
  The function {\tt computeStates(k)} computes the
  bindings of pointer variables to the access paths using
  traditional iterative data flow analysis.  The access
  paths are either symbolic locations, e.g. $\loc$, or
  symbolic location followed by pointer fields, e.g.
  $\loc\rightarrow$\emph\mynext$\rightarrow$\emph{\tt
    next}. To guarantee termination, we limit the length of
  access paths to {\tt k}, that is a parameter to {\tt
    computeStates}. A special summary field `{$*$}' is used
  to limit the access paths. which stands for any field
  dereferenced beyond length {\tt k}. Hence, for {\tt k =
    1}, all the access paths in the set
  \{\loc$\rightarrow${\tt next}$\rightarrow$\mynext,
  \loc$\rightarrow${\tt
    next}$\rightarrow$\mynext$\rightarrow$\mynext,
  \loc$\rightarrow$\mynext$\rightarrow${\tt
    next}$\rightarrow$\mynext$\cdots\rightarrow${\tt next}\}
  can be abstracted as a single summarized path
  \loc$\rightarrow$\mynext$\rightarrow{*}$. Similarly,
  assuming a data structure has two reference fields
  \lt\ and \rt, the summarized path
  $\loc\rightarrow\lt\rightarrow\rt\rightarrow{*}$ could
  stand for any of the access paths
  $\loc\rightarrow\lt\rightarrow\rt\rightarrow\lt,\ \loc\rightarrow\lt\rightarrow\rt\rightarrow\rt,\ \loc\rightarrow\lt\rightarrow\rt\rightarrow\lt\rightarrow\lt,\ \loc\rightarrow\lt\rightarrow\rt\rightarrow\lt\rightarrow\rt$
  and more such paths.
  
  Given a variable to access path mapping {\tt States}, {\tt
    computeReadWrite(States)} computes read and 
  write sets for each statement in the program in a single
  pass. This information can be used for computation of various
  dependencies (flow, anti
  or output). Let $S$ and $S'$ be statements in the program
such that there exists an execution path from $S$ to
$S'$. Then, the dependence of $S'$ on $S$ is computed as
follows: \\
 %in Figure~\ref{fig:depcomp}.
\newcommand{\flowdep}{\mbox{flow-dep}}
\newcommand{\antidep}{\mbox{anti-dep}}
\newcommand{\outputdep}{\mbox{output-dep}}
\newcommand{\rs}[1]{\mbox{read}(#1)}
\newcommand{\ws}[1]{\mbox{write}(#1)}
\newcommand{\rst}[2]{\mbox{read}(#1, #2)}
\newcommand{\wst}[2]{\mbox{write}(#1, #2)}
\newcommand{\set}{\mbox{set}}
\noindent

$\begin{array}{rcl}
\interf{\set_1}{\set_2} &\equiv& \isInterfering(p, \alpha, q,
\beta) \; \mbox{where } 
p.\alpha \in \set_1   \wedge  q.\beta \in \set_2 \\ 
\flowdep(S, S') &\equiv& \interf{\ws{S}}{\rs{S'}} \\ 
\antidep(S, S') &\equiv& \interf{\rs{S}}{\ws{S'}} \\
\outputdep(S, S') &\equiv& \interf{\ws{S}}{\ws{S'}}
\end{array}$ \\

\noindent where \isInterfering\ is the function provided by
shape analysis.

\newcommand{\mapp}{$\equiv$}
\begin{table}
\centering
\begin{tabular}{|c|l|c|c|c|}
\hline 
Stmt & Variables and Locations of Interest & Read Set & Write
Set \\ 
\hline
S1 & p  \mapp\  \{$\loc$\} & $\emptyset$ & $\emptyset$ \\
S2 & q \mapp\   \{$\loc\rightarrow$\mynext, $\loc\rightarrow$\mynext$\rightarrow *$\}& $\emptyset$ & $\emptyset$ \\
S3 & q \mapp\  \{$\loc\rightarrow$\mynext, $\loc\rightarrow$\mynext$\rightarrow *$\} & \{$\loc\rightarrow$\mynext, $\loc\rightarrow$\mynext$\rightarrow *$\} & $\emptyset$ \\
S4 & r \mapp\   \{$\loc\rightarrow$\mynext$\rightarrow *$\} & $\emptyset$ & $\emptyset$ \\
S5 & r \mapp\ \{$\loc\rightarrow$\mynext$\rightarrow *$\} & $\emptyset$ & \{$\loc\rightarrow$\mynext$\rightarrow *$\} \\
S6 & p \mapp\ \{$\loc\rightarrow$\mynext$\rightarrow *$\} & $\emptyset$ & $\emptyset$ \\
\hline
\end{tabular}

\noindent \loc\ is the symbolic location representing the
value of the variable {\tt list} at the start of the program.
\caption{Simple dependence analysis for code in
  Figure~\ref{fig:motiv}} 
\label{fig:table_simple_dep}
\hrule
\end{table}

\begin{example} {\label{ex:dep}\rm
Table~\ref{fig:table_simple_dep} shows the state (pointer
variables and symbolic memory locations referred by the
variables) and the read and write sets for each statement in
the example code of Figure~\ref{fig:motiv}. From the table,
we can infer the following dependences:
\begin{enumerate}
\item loop independent anti-dependence  from statement {S3} to
statement {S5}
\item loop carried flow-dependence  from statement {S5} to
statement {S3}
\end{enumerate}  
Next we explain how we can further refine our dependence
analysis to filter out some spurious dependences.
}
\hfill\psframebox{}  \end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Loop Dependence Analysis}
As can be seen from the Example~\ref{ex:dep} above, our
approach, as explained earlier, does not work well for
loops. This is because it combines the paths being accessed
in different iterations of a loop. To get better result in
presence of loops we need to keep the accesses made by
different iterations of a loop separate. To do so, we have
devised another novel approach, which works as follows: Given
a loop, we first identify the navigators\footnote{a navigator
  consists of a pointer variable and a set of field
  references that are used to traverse a data structure
  inside the loop.}~\cite{ghiya98detecting} for the loop,
then by a single symbolic traversal over the loop, we compute
the read and write accesses made by each statement in terms
of the values of the navigators. The read and write sets thus
obtained are generalized to represent arbitrary iteration of
the loop, using the iteration number as a parameter. 

Let $S$ and $S'$ be two statements inside a loop. Further,
let $\wst{S}{i}$ denote the set of access paths written by
statement $S$ in the iteration number $i$, and let
$\rst{S'}{j}$ denote the set of access paths read by
statement $S'$ in the iteration number $j$. Then, 
\begin{itemize}
\item $S'$ is loop independent flow dependent on $S$ if there
  is an execution path from $S$ to $S'$ that does not cross
  the loop boundary and there exist $i$ within loop
  bounds\footnote{in case loop bounds can not be computed at
    compile time, we can assume them to be $(-\infty,
    \infty)$} such that $\interf{\wst{S}{i}}{\rst{S'}{i}}$ is
  true.
\item $S'$ is loop dependent flow dependent on $S$ if there
  exist $i$ and $j$ within loop bounds such that $j > i$, and
  $\interf{\wst{S}{i}}{\rst{S'}{j}}$ is true.
\end{itemize}

We can similarly define loop independent and loop carried
anti-dependence and flow-dependence. The following example
explains our approach for the loop dependence detection.

\begin{figure}[t]
  \begin{center}
    \scalebox{.85}{\begin{tabular}{@{}c@{}|@{}c@{}}
      \input{fig_code_no_dep}
      &
      \input{fig_code_dep} \\
      {(a) Loop without Dependence.} &
      {(b) Loop with Dependence.}
    \end{tabular}}
  \end{center}
  \caption{\label{fig:loopdep} Identifying Loop Dependences}
\hrule
\end{figure}
\begin{example}\label{ex:loopdeps}{\rm 
    Consider Figure~\ref{fig:loopdep}. Assume in each case
    \loc\ is the symbolic variable pointed to by the variable
    {\tt list}.
             
    For the code in Figure~\ref{fig:loopdep}(a), the
    navigator is $\langle\loc,
    \mynext\rightarrow\mynext\rangle$. Using $i$ to represent
    the iteration number, the generalized access path read by
    S11 is \loc$\rightarrow\mynext^{2i}\rightarrow\mynext$ and
    the generalized access path written by S12 is
    \loc$\rightarrow\mynext^{2i+1}\rightarrow\mynext$ Clearly
    there is no loop independent dependence. To find out loop
    carried dependence, we have to find out whether for
    iterations $i$ and $j$, the two paths point to the same
    heap location. This reduces to finding out if there is a
    possible solution to the following equation:
      \[ \loc\rightarrow\mynext^{2i}\rightarrow\mynext  =
      \loc\rightarrow\mynext^{2j+1}\rightarrow\mynext \] 
      In other words, we have to find out if  integer solutions
      to the following equation are possible :
      \[2*i  = 2*j + 1\]
      GCD~\cite{Kennedy01Optimizing} or
      Lamport~\cite{Lamport1974parallel} test tell us that
      this equation can not have integer solutions. Thus,
      there is no dependence among the statements.

      For the code in Figure~\ref{fig:loopdep}(b), the navigator
      is $\langle\loc,
    \mynext\rangle$. In this case the equation to find 
      out the loop carried dependences among statements S21
      and S22 reduces to : 
      \[ i = j + 1\]
      which has integer solutions. So we have to
      conservatively report dependence between the two statements

In both the cases, we also need the shape analyzer to assert
that there is no sharing within the underlying data
structure. Had there been a sharing, we would have to report
conservatively that there exists a dependence.  }
\end{example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
Data dependence analysis on static and stack related data
structure is well explored in
literature~\cite{Allen87automatic, Wolf91loop,
  Banerjee93automatic, Kennedy01Optimizing}. Our approach
extends the work to handle heap data structure.  Earlier work
for heap induced data dependence detection
includes~\cite{horwitz89dependence,Hendren90parallel,
  ghiya98detecting, navarro05dependence}.  The work by
Hendren et al.~\cite{Hendren90parallel} approximates the
access paths between pointers using {\em Path Matrix}, while
Ghiya et. al.~\cite{ghiya98detecting} use direction matrix
and interference matrix to realize the shape
information. This shape information is used for coarse
characterization of the underlying data structure as Tree,
DAG or Cycle. As noted earlier, our shape analysis framework
is more precise than theirs as we can take care of
destructive updates in a better way.  Navarro et. al.
~\cite{navarro05dependence} propose a dependence test which
intermixes shape analysis and dependence analysis
together. During the analysis, the abstract structure of the
dynamically allocated data is computed and is also tagged
with read/write tags to find out dependencies. The resulting
analysis is very precise, but it is costly. Further their
shape analysis component is tightly integrated within the
dependence analysis, while in our approach we keep the
  two separate as it gives us modularity and the scope to
  improve the precision of our dependence analysis by using a
  more precise shape analysis, if available.  Our approach
is closest to the technique proposed by Horwitz
et. al.~\cite{horwitz89dependence}. They also associate read
and write sets with each program statement to detect heap
dependencies. They have also proposed technique to compute
dependence distances for loop constructs. However, there
technique requires iterating over a loop till a fixed point
is reached, which is different from our method of computing
loop dependences as a set of equations in a single pass, and
then solving these equations using classical tests.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion and Future Work}

In this report we have presented our work on heap dependence
analysis that can be utilized by a parallelizing compiler to
parallelize sequential programs. Our method is divided into
three phases - the shape analysis phase, the state
computation phase, and the loop analysis phase, with
carefully chosen interfaces between phases to combine work
done by individual phases. This gives us the flexibility to
work on testing and improving each phase independently.  Our
loop dependence analysis abstracts the dependence information
in forms of linear equations, that can be solved using
traditional dependence analysis
tests~\cite{Allen87automatic,Wolf91loop,Banerjee93automatic}
that exist for finding array dependences.

Our analysis is intra procedural, and we use conservative
approximation of function calls assuming worst case scenario.
Our next challenge is to develop an inter procedural analysis
to handle function calls more precisely. We have to further
develop our shape analysis technique and the loop analysis to
handle more of frequently occurring programming patterns to find
precise dependences for these patterns.  We  also want to
improve our summarization technique. Earlier we have used
graph based approximations of access
paths~\cite{khedker07heap} to compute liveness of heap
data. We plan to explore if the same summarization technique
can also be applied here.

Finally, to show that our analysis is practical, we are
developing a prototype model using GCC
framework~\cite{gcc-web} to show the effectiveness on large
benchmarks. However, this work is still in very early stages.

\bibliography{parrefs}

\end{document}
